{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marek-bardonski/airev-advanced-workshops/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn_KXjFSemkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AI REV LLC - Copyrights 2020\n",
        "# [Google Colab] Change runtime mode to GPU\n",
        "# https://docs.rapids.ai/\n",
        "# https://alraqmiyyat.github.io/2013/01-02.html\n",
        "\n",
        "!nvcc --version\n",
        "!pip3 install wget\n",
        "\n",
        "# https://github.com/rapidsai/cudf/issues/3390\n",
        "!pip3 install pyarrow==0.15.0 ## Workaround to cover up for Google Colab bug\n",
        "\n",
        "#!pip3 install cudf-cuda100\n",
        "#!pip3 install nvstrings-cuda100\n",
        "#!pip3 install cuml-cuda100\n",
        "#!pip3 install nvvm-cuda100\n",
        "\n",
        "# RAPIDS installation script. Thanks Ritchie Ng and NVIDIA Corporation.\n",
        "# intall miniconda\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# install RAPIDS packages\n",
        "!conda install -q -y --prefix /usr/local -c conda-forge \\\n",
        "  -c rapidsai-nightly/label/cuda10.0 -c nvidia/label/cuda10.0 \\\n",
        "  cudf cuml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_feLB_gSiQo4",
        "colab_type": "text"
      },
      "source": [
        "**Remember to restart the runtime at this moment and restart the procedure from the top.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET1uPRNm4TSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set environment vars\n",
        "import sys, os, shutil\n",
        "\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
        "sys.path.append('/usr/local/')\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "\n",
        "# copy .so files to current working dir\n",
        "for fn in ['libcudf.so', 'librmm.so']:\n",
        "  shutil.copy('/usr/local/lib/'+fn, os.getcwd())\n",
        "\n",
        "import wget\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "import cudf\n",
        "import sys, os\n",
        "import nvcategory\n",
        "import os\n",
        "import numpy as np\n",
        "import nvstrings\n",
        "import nltk\n",
        "from numba import cuda\n",
        "import json\n",
        "import nvtext\n",
        "import ctypes\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import cupy\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOFllMJm9Pe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmPAza7yBe5z",
        "colab_type": "text"
      },
      "source": [
        "**If you got import errors, please look above or ask instructor.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBx-VbqMYEBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Raw datasets are samples from WebHose.io \n",
        "print('Beginning dataset download with wget module')\n",
        "\n",
        "url = 'https://bardonski.pl/chineese.zip'\n",
        "wget.download(url)\n",
        "url = 'https://bardonski.pl/arabic.zip'\n",
        "wget.download(url)\n",
        "url = 'https://bardonski.pl/arabic-true-pr.csv'\n",
        "wget.download(url)\n",
        "# TODO Arabic-2\n",
        "# TODO Chineese-1\n",
        "# TODO Chineese-2\n",
        "\n",
        "print('Beginning word vectors download with wget module')\n",
        "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz'\n",
        "print('Word vectors unzip')\n",
        "wget.download(url)\n",
        "!gzip -d cc.ar.300.vec.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDlFmOkAKDBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip = ZipFile('chineese.zip')\n",
        "zip.extractall()\n",
        "\n",
        "!mkdir chineese\n",
        "\n",
        "zip = ZipFile('630_webhose-2016-10_20170904084325.zip')\n",
        "zip.extractall('chineese')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G37zDeifXwLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip = ZipFile('arabic.zip')\n",
        "zip.extractall()\n",
        "\n",
        "!mkdir arabic\n",
        "\n",
        "zip = ZipFile('627_webhose-2016-10_20170904083346.zip')\n",
        "zip.extractall('arabic')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLyn-jT-Y3JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Expected 236384 arabic articles\n",
        "#Expected 316004 chineese articles\n",
        "!ls arabic -l | wc -l\n",
        "!ls chineese -l | wc -l\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjkkjECXGYsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbriaZfeaKr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thanks VibhuJawa\n",
        "def get_text(lines):\n",
        "    \"\"\"\n",
        "        returns non empty lines from a list of lines\n",
        "    \"\"\"\n",
        "    decoded = json.loads(lines[0])\n",
        "    clean_lines = decoded['text']\n",
        "    return [clean_lines]\n",
        "\n",
        "def get_txt_lines(data_dir):\n",
        "    \"\"\"\n",
        "        Read text lines from gutenberg tests\n",
        "        returns (text_ls,fname_ls) where \n",
        "        text_ls = input_text_lines and fname_ls = list of file names\n",
        "    \"\"\"\n",
        "    text_ls = []\n",
        "    fname_ls = []\n",
        "    for fn in os.listdir(data_dir):\n",
        "        full_fn = os.path.join(data_dir,fn)\n",
        "        with open(full_fn,encoding=\"utf-8\",errors=\"ignore\") as f:\n",
        "            content = f.readlines()\n",
        "            content = get_text(content)\n",
        "            if content is not None:\n",
        "                text_ls += content\n",
        "                ### dont add .txt to the file\n",
        "                fname_ls += [fn[:-4]]*len(content)\n",
        "        #return text_ls, fname_ls    \n",
        "    \n",
        "    return text_ls, fname_ls    \n",
        "    \n",
        "print(\"File Read Time:\")\n",
        "%time txt_ls,fname_ls = get_txt_lines('arabic')\n",
        "df = cudf.DataFrame()\n",
        "\n",
        "print(\"\\nCUDF  Creation Time:\")\n",
        "%time df['text'] = nvstrings.to_device(txt_ls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBIamb5gbd_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of lines in the DF = {:,}\".format(len(df)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fheBQLXhr-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(10).to_pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEaGqGwcy6Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STOPWORDS = nltk.corpus.stopwords.words('arabic')\n",
        "\n",
        "filters = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>',\n",
        "           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\~', '\\t','\\\\n',\"'\",\",\",'~' , '—']\n",
        "\n",
        "def preprocess_text(input_strs , filters=None , stopwords=STOPWORDS):\n",
        "    \"\"\"\n",
        "        * filter punctuation\n",
        "        * to_lower\n",
        "        * remove stop words (from nltk corpus)\n",
        "        * remove multiple spaces with one\n",
        "        * remove leading spaces    \n",
        "    \"\"\"\n",
        "    \n",
        "    # filter punctuation and case conversion\n",
        "    input_strs = input_strs.str.replace_multi(filters, ' ', regex=False)\n",
        "    input_strs = input_strs.str.lower()\n",
        "        \n",
        "    # remove stopwords\n",
        "    stopwords_gpu = nvstrings.to_device(stopwords)\n",
        "    input_strs = nvtext.replace_tokens(input_strs.data, stopwords_gpu, ' ')\n",
        "    input_strs = cudf.Series(input_strs)\n",
        "        \n",
        "    # replace multiple spaces with single one and strip leading/trailing spaces\n",
        "    input_strs = input_strs.str.replace(r\"\\s+\", ' ', regex=True)\n",
        "    input_strs = input_strs.str.strip(' ')\n",
        "    \n",
        "    return input_strs\n",
        "\n",
        "def preprocess_text_df(df, text_cols=['text'], **kwargs):\n",
        "    for col in text_cols:\n",
        "        df[col] = preprocess_text(df[col], **kwargs)\n",
        "    return  df\n",
        "\n",
        "%time df = preprocess_text_df(df, filters=filters)\n",
        "\n",
        "# TASK #1 - Remove stopwords keeping the arabic symbols. Hint: ^[\\u0621-\\u064A0-9 ]+$\n",
        "# TASK #2 - Shuffle the DataFrame df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FNhnzVazYi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(5).to_pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oBSnlWUz2o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAMPLE_SIZE = 1000\n",
        "df2 = df.head(SAMPLE_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w64RIm__DRRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.to_pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHgMOTlfDSsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many articles contain the word bitcoin?\n",
        "sum(df2['text'].str.find('بيتكوين')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrjDHohlZxdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_df = cudf.read_csv(\"cc.ar.300.vec\",\n",
        "                       header=None,\n",
        "                       delim_whitespace=True,\n",
        "                       quoting=3,\n",
        "                       skiprows=1)  #ignore quoting\n",
        "print(pre_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRoYzXaB521k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the file with cudf\n",
        "names = ['query', 'title', 'text', 'link', 'desc','other']\n",
        "# Note 'int' for 3rd column- text will be hashed\n",
        "dtypes = ['str', 'str', 'str', 'str', 'str', 'str']\n",
        "df_pos = cudf.read_csv('arabic-true-pr.csv', delimiter=',',\n",
        "                   names=names, dtype=dtypes,\n",
        "                   skiprows=1)\n",
        "df_pos.head(15).to_pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o-PIM-H6bNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pos = df_pos.drop(['query', 'title', 'link', 'desc', 'other'])\n",
        "df_pos.add_column('target', 1)\n",
        "df_pos = df_pos.dropna()\n",
        "df2.add_column('target', 0)\n",
        "df2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r04jGBfK9G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = cudf.core.reshape.concat([df_pos, df2], 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8YAeHzg701a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thanks Ayush Kumar\n",
        "# setting the max length of each article to 200\n",
        "MAX_LEN = 200\n",
        "num_sents = df['text'].data.size()\n",
        "\n",
        "# generate the tokens\n",
        "seq = df['text'].data.split_record(' ')\n",
        "# padding each strings if smaller or trim down if larger\n",
        "for i in range(len(seq)):\n",
        "  l = seq[i].size()\n",
        "  if l<= MAX_LEN:\n",
        "    seq[i] = seq[i].add_strings(nvstrings.to_device((MAX_LEN-l)*['PAD']))\n",
        "  else:\n",
        "    seq[i] = seq[i].remove_strings(list(range(MAX_LEN,l)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR4_RAaLN1uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(seq[40])\n",
        "print(seq[4])\n",
        "print((len(seq)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T54I5yyN_kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating the indices corresponding each token \n",
        "c = nvcategory.from_strings_list(seq)\n",
        "print(c.keys_size())   # total number of unique tokens\n",
        "print(c.size())       # total number of tokens or vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi4M5jLJPHPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating gdf using unique tokens\n",
        "# TASK more preprocessing - that can be tricky in Arabic\n",
        "sent_df = cudf.DataFrame({'tokens':c.keys()})\n",
        "sent_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5vOj868PNk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preparing the X_train \n",
        "X_train = cuda.device_array((num_sents, MAX_LEN), dtype=np.int32)\n",
        "c.values(X_train.device_ctypes_pointer.value)\n",
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU1jkxGZYubM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preparing the y_train\n",
        "y_train = df['target'].astype('float32').to_gpu_array()\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxYdgJU1Qj7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating embedding matrix \n",
        "vocab_df = sent_df.merge(pre_df,\n",
        "                         left_on='tokens',\n",
        "                         right_on='0',\n",
        "                         how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlerzfKbQ1M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_token = vocab_df.shape[0]\n",
        "print(all_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L448hiT39uDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_df.drop_column('0')\n",
        "vocab_df.drop_column('tokens')\n",
        "\n",
        "# filling the not found tokens with random vector\n",
        "for c in vocab_df.columns:\n",
        "  vocab_df[c] = vocab_df[c].fillna(cupy.random.normal(size=all_token)).astype(np.float32)\n",
        "\n",
        "# embedding matrix\n",
        "vocab = vocab_df.as_gpu_matrix(order='C')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOQqgbLD-Fwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open issue #23067\n",
        "def devndarray2tensor(dev_arr, dtyp='float32'):\n",
        "    dmap = {'float32':torch.float32, 'int32':torch.int32}\n",
        "    t = torch.empty(size=dev_arr.shape, dtype=dmap[dtyp]).cuda()\n",
        "    ctx = cuda.cudadrv.driver.driver.get_context()\n",
        "    \n",
        "    # constant value of #bytes in float32 = 4\n",
        "    mp = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_ulong(t.data_ptr()), t.numel()*4)\n",
        "    tmp_arr = cuda.cudadrv.devicearray.DeviceNDArray(t.size(), [i*4 for i in t.stride()], np.dtype(dtyp), \n",
        "                                            gpu_data=mp, stream=torch.cuda.current_stream().cuda_stream)\n",
        "    tmp_arr.copy_to_device(dev_arr)\n",
        "    return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WBqmq1YfvVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Workshop goal -> Find as many as possible articles, that have a indication of bitcoin price going up or down, that would be usefull for a trader.\n",
        "\n",
        "# Recommendations\n",
        "'''\n",
        "Literature:\n",
        "1. https://roywrightme.wordpress.com/2017/11/16/positive-unlabeled-learning/\n",
        "2. http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\n",
        "\n",
        "1. Firstly, try to clean the positive dataset. You can use free Google Tranlsate API to understand the articles. \n",
        "2. Using the clean dataset, try to train a simple model like logistic regression over glove embeddings and the PU technique\n",
        "3. Using the above model, try to find additional positive articles, that rank high in the above.\n",
        "4. Having additional data, train a more complex model capable of upgrading with Variational Dropout Uncretainty estimation. Recommended example is 1D Convolution. \n",
        "5. By performing na inference on the unlabelled dataset, try to find additional positive sample with low aleatoric uncertainty."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xlXn7mlaksh",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}